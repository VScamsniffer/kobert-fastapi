import numpy as np
np.bool = bool
import torch
from transformers import BertModel, BertTokenizer
from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.responses import JSONResponse
import os
from kobert_tokenizer import KoBERTTokenizer
from kobert.pytorch_kobert import get_pytorch_kobert_model
import torch.nn as nn
import whisper
from pydub import AudioSegment
from pydub.utils import which
import logging
from typing import Optional, List
from functools import lru_cache
from collections import deque
import asyncio
from datetime import datetime, timedelta
import asyncio

app = FastAPI()
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì„¤ì •
BATCH_SIZE = 32
BATCH_TIMEOUT = 0.3  # ì´ˆ
MAX_QUEUE_SIZE = 100

# ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ íì™€ ì´ë²¤íŠ¸
class BatchProcessor:
    def __init__(self):
        self.queue = deque()
        self.event = asyncio.Event()
        self.processing = False
        self.last_process_time = datetime.now()

    async def add_to_queue(self, item):
        if len(self.queue) >= MAX_QUEUE_SIZE:
            raise HTTPException(status_code=503, detail="ì„œë²„ê°€ ë„ˆë¬´ ë§ì€ ìš”ì²­ì„ ì²˜ë¦¬ì¤‘ì…ë‹ˆë‹¤")
        
        future = asyncio.Future()
        self.queue.append((item, future))
        
        if not self.processing:
            asyncio.create_task(self.process_batch())
        
        return await future

    async def process_batch(self):
        self.processing = True
        
        while self.queue:
            current_time = datetime.now()
            
            # ë°°ì¹˜ í¬ê¸°ë‚˜ ì‹œê°„ ì´ˆê³¼ì— ë„ë‹¬í•  ë•Œê¹Œì§€ ëŒ€ê¸°
            if (len(self.queue) < BATCH_SIZE and 
                (current_time - self.last_process_time).total_seconds() < BATCH_TIMEOUT):
                await asyncio.sleep(0.01)
                continue
                
            # í˜„ì¬ ë°°ì¹˜ ì²˜ë¦¬
            batch = []
            futures = []
            batch_size = min(BATCH_SIZE, len(self.queue))
            
            for _ in range(batch_size):
                item, future = self.queue.popleft()
                batch.append(item)
                futures.append(future)
            
            try:
                # ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰
                results = await process_text_batch(batch)
                
                # ê²°ê³¼ ë°˜í™˜
                for future, result in zip(futures, results):
                    future.set_result(result)
                    
            except Exception as e:
                # ì—ëŸ¬ ì²˜ë¦¬
                for future in futures:
                    future.set_exception(e)
            
            self.last_process_time = datetime.now()
        
        self.processing = False

# ê¸°ì¡´ BERTClassifier í´ë˜ìŠ¤ëŠ” ë™ì¼í•˜ê²Œ ìœ ì§€
class BERTClassifier(nn.Module):
    def __init__(self, hidden_size=768, num_classes=2):
        super(BERTClassifier, self).__init__()
        self.bert, _ = get_pytorch_kobert_model()
        self.classifier = torch.nn.Linear(768, 1)

    def forward(self, token_ids, valid_length, segment_ids):
        _, pooled_output = self.bert(input_ids=token_ids, return_dict=False)
        return self.classifier(pooled_output)

# ëª¨ë¸ ì´ˆê¸°í™”
MODEL_PATH = os.path.join(os.path.dirname(__file__), "kobert_state_dict2.pth")
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

TEMP_DIR = os.path.join(os.getcwd(), "temp_files")
os.makedirs(TEMP_DIR, exist_ok=True)

@lru_cache(maxsize=1)
def load_models():
    model = BERTClassifier()
    model.load_state_dict(torch.load(MODEL_PATH, map_location=device))
    model.to(device)
    model.eval()
    
    tokenizer = BertTokenizer(vocab_file="tokenizer_vocab.txt", do_lower_case=False)
    whisper_model = whisper.load_model("base", device=device)
    
    return model, tokenizer, whisper_model

model, tokenizer, whisper_model = load_models()
batch_processor = BatchProcessor()

# ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ í•¨ìˆ˜
async def process_text_batch(texts: List[str]) -> List[float]:
    """ë°°ì¹˜ë¡œ í…ìŠ¤íŠ¸ ë¶„ì„ ì²˜ë¦¬"""
    try:
        # í† í°í™”
        inputs = tokenizer(
            texts,
            padding=True,
            truncation=True,
            max_length=512,
            return_tensors="pt"
        )
        
        # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        inputs = {k: v.to(device) for k, v in inputs.items()}
        valid_lengths = torch.tensor([len(ids) for ids in inputs["input_ids"]]).to(device)
        segment_ids = torch.zeros_like(inputs["input_ids"]).to(device)

        # ë°°ì¹˜ ì¶”ë¡ 
        with torch.no_grad():
            outputs = model(inputs["input_ids"], valid_lengths, segment_ids)
            probabilities = torch.sigmoid(outputs.squeeze(1)).cpu().numpy().tolist()

        return probabilities

    except Exception as e:
        logger.error(f"ğŸš¨ [ERROR] ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return [0.0] * len(texts)

@app.post("/upload-audio/")
async def upload_audio_file(file: UploadFile = File(...)):
    """ìŒì„± íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ë³€í™˜ í›„ ë¶„ì„"""
    logger.info("[ìš”ì²­] íŒŒì¼ ì—…ë¡œë“œ ë° ë¶„ì„ ìš”ì²­")

    ext = os.path.splitext(file.filename)[1].lower()
    allowed_extensions = [".mp3", ".wav", ".ogg", ".m4a"]

    if ext not in allowed_extensions:
        raise HTTPException(status_code=400, detail="ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤.")

    try:
        # íŒŒì¼ ì²˜ë¦¬
        temp_filename = f"{os.urandom(8).hex()}{ext}"
        temp_audio_path = os.path.join(TEMP_DIR, temp_filename)
        
        contents = await file.read()
        with open(temp_audio_path, "wb") as f:
            f.write(contents)

        # WAV ë³€í™˜
        wav_file_path = os.path.join(TEMP_DIR, f"{os.path.splitext(temp_filename)[0]}.wav")
        if ext != '.wav':
            logger.info(f"[ğŸ™ï¸ ë³€í™˜] {ext} â†’ WAV ë³€í™˜ ì¤‘...")
            audio = AudioSegment.from_file(temp_audio_path, format=ext[1:])
            audio.export(wav_file_path, format="wav", parameters=["-ac", "1", "-ar", "16000"])
        else:
            wav_file_path = temp_audio_path

        # STT ìˆ˜í–‰
        text = await audio_to_text(wav_file_path)
        if isinstance(text, str) and text.startswith("Whisper ë³€í™˜ ì‹¤íŒ¨"):
            raise HTTPException(status_code=500, detail=text)

        logger.info(f"[ë¶„ì„í•  í…ìŠ¤íŠ¸]: {text}")

        # ë°°ì¹˜ ì²˜ë¦¬ íì— ì¶”ê°€
        probability = await batch_processor.add_to_queue(text)

        # ì„ì‹œ íŒŒì¼ ì •ë¦¬
        try:
            os.remove(temp_audio_path)
            if ext != '.wav':
                os.remove(wav_file_path)
        except Exception as e:
            logger.warning(f"ì„ì‹œ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")

        return {"probability": probability * 100, "text": text}

    except Exception as e:
        logger.error(f"ğŸš¨ [ERROR] ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

async def audio_to_text(wav_file_path: str) -> str:
    """ì˜¤ë””ì˜¤ ë¶„ì„"""
    if not os.path.exists(wav_file_path):
        raise FileNotFoundError(f"File not found: {wav_file_path}")

    try:
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(
            app.state.thread_pool, 
            lambda: whisper_model.transcribe(
                wav_file_path,
                fp16=False,
                language='ko'
            )
        )
        return result["text"]
    except Exception as e:
        logger.error(f"ğŸš¨ [ì˜¤ë¥˜] Whisper ë³€í™˜ ì‹¤íŒ¨: {str(e)}")
        return f"Whisper ë³€í™˜ ì‹¤íŒ¨: {str(e)}"

@app.on_event("startup")
async def startup_event():
    """ì‚¬ì´íŠ¸ ì‹œì‘ì‹œ ëª¨ë¸ë§ ëŒ€ê¸°ì‹œí‚¤ê¸°"""
    import concurrent.futures
    app.state.thread_pool = concurrent.futures.ThreadPoolExecutor(max_workers=4)
    
    # ë”ë¯¸ë°ì´í„°ë¡œ ëª¨ë¸ë§ ëŒ€ê¸°ì‹œí‚¤ê¸°
    dummy_text = "ì•ˆë…•í•˜ì„¸ìš”"
    await batch_processor.add_to_queue(dummy_text)
    logger.info("Models warmed up successfully")

@app.on_event("shutdown")
async def shutdown_event():
    """ìŠ¤ë ˆë“œ ì •ë¦¬"""
    app.state.thread_pool.shutdown()

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="127.0.0.1", port=8000)
